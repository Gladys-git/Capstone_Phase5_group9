{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62b81de",
   "metadata": {},
   "source": [
    "# BUSINESS UNDERSTANDING\n",
    "\n",
    "## PROBLEM STATEMENT\n",
    "\n",
    "Customer churn significantly impacts revenue and customer lifetime value for internet service providers. Retaining existing customers is more cost-effective than acquiring new ones. However, identifying which customers are likely to churn remains a challenge.\n",
    "\n",
    "> This project aims to develop a *predictive machine learning model* to estimate churn risk scores and identify high-risk customers using a mix of behavioral, demographic, and feedback data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bba5fc",
   "metadata": {},
   "source": [
    "## OBJECTIVES\n",
    "\n",
    "To build a supervised machine learning model capable of predicting customer churn risk for internet service providers using behavioral, transactional, and feedback data.\n",
    "\n",
    "> *Goal*: Help companies identify customers at high risk of churning, enabling timely and targeted retention strategies.\n",
    "\n",
    "This analysis will aim to answer the following business questions:\n",
    "\n",
    "- Which *customer segments* are most likely to churn?\n",
    "- What are the top *behavioral or demographic* predictors of churn?\n",
    "- Can *sentiment in customer feedback* help predict churn risk?\n",
    "- How effective are current *loyalty strategies* in retaining customers?\n",
    "- What *actionable retention strategies* can we derive from high churn risk profiles?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1143bc",
   "metadata": {},
   "source": [
    "## BUSINESS BENEFITS\n",
    "\n",
    "### 1. Improved Customer Retention\n",
    "- Early identification of likely-to-churn customers allows for *timely intervention*.\n",
    "\n",
    "### 2. Cost Efficiency\n",
    "- Reduced marketing spend on acquiring new customers by *retaining existing ones*.\n",
    "\n",
    "### 3. Customer Lifetime Value (CLV) Optimization\n",
    "- Retaining *high-value customers* increases CLV and long-term profitability.\n",
    "\n",
    "### 4. Data-Driven Decision Making\n",
    "- Marketing and customer success teams can rely on churn predictions and insights to *refine strategies*.\n",
    "\n",
    "### 5. Product Improvement\n",
    "- Insights from churn predictors and feedback sentiment can inform *service enhancements* and *innovation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f2742",
   "metadata": {},
   "source": [
    "##  SUCCESS CRITERIA\n",
    "\n",
    "### Technical Success\n",
    "- A cleaned, well-structured dataset ready for modeling.\n",
    "- A classification model (baseline + advanced) achieving *≥ 70% accuracy*.\n",
    "- Evaluation metrics showing *balanced performance,* with *recall prioritized*.\n",
    "\n",
    "### Business Success\n",
    "- Identification of *key churn drivers* for actionable insights.\n",
    "- *Recommendations* for personalized customer retention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d30c42",
   "metadata": {},
   "source": [
    "# DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c0dec5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, accuracy_score\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrixDisplay\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a832db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from absolute path\n",
    "df = pd.read_csv(\"customer_churn.csv\")\n",
    "#load the first 5 columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2660da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the shape of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all columns that exist in our dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff598937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data types in our dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e41492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of values in the dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5e1a0",
   "metadata": {},
   "source": [
    "Standardizing the column names by converting them to lowercase and replacing spaces with underscores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8725082",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920a996",
   "metadata": {},
   "source": [
    "Calculating percentages of null values on each column. This is to determine the best approach of dealing with the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc305d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a454ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='PuBu')\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bad202",
   "metadata": {},
   "source": [
    "This chart shows that region_category and points_in_wallet have the highest amount of missing values. preferred_offer_types has small number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate null values per column as a percentage\n",
    "null_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Display the result\n",
    "print(\"Percentage of null values per column:\")\n",
    "print(null_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6300ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "sns.countplot(x='churn_risk_score', data=df, palette='Set1')\n",
    "plt.title(\"Churn Risk Score Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299e7ee",
   "metadata": {},
   "source": [
    "The chart shows that category 1 churn_risk_score has slightly more values than category 0 churn_risk_score. This tells us that more people are likely to leave the company than to stay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f97230",
   "metadata": {},
   "source": [
    "We will use a heatmap to show correlation between our numerical variables. We can use it to see which features (like `tenure_days` or `feedback_sentiment`) are most connected to `churn_risk_score` when picking model inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(numerical_features.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e823f",
   "metadata": {},
   "source": [
    "We observe that there is no strong correlation between the numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52859289",
   "metadata": {},
   "source": [
    "We will use a histogram to show how customer wallet points are spread out. The plot will help us understand if most customers have low, average, or high points. It is useful for identifying outliers or patterns that may affect churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our date column has dates recorded in different formats. To convert to datetime, we will use a date_parser\n",
    "date_parser = lambda x: pd.to_datetime(x, format='mixed', dayfirst=True)\n",
    "df['joining_date'] = df['joining_date'].apply(date_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2b70d",
   "metadata": {},
   "source": [
    "To facilitate the analysis of seasonal and monthly churning patterns, we will extract the month and year from this column into separate fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering a column for month and year\n",
    "df['joining_year'] = df['joining_date'].dt.year\n",
    "df['joining_month'] = df['joining_date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d13335",
   "metadata": {},
   "source": [
    "We are going to check number of signups over the period the data was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a4f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Signups Over Time\n",
    "# bar plot of number of users joining per year.\n",
    "df.groupby('joining_year').size().plot(kind='barh', title='Customer Signups per Year');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11aa6c",
   "metadata": {},
   "source": [
    "From the above bargraph we can see that customer signups averaged at slightly over 12,000 with the year 2017 having highest number among the 3 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd633b",
   "metadata": {},
   "source": [
    "We will check monthly signup trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec13892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly Signup Trend (across all years)\n",
    "# Group by joining_month to see seasonality.\n",
    "df.groupby('joining_month').size().plot(kind='barh', title='Monthly Signup Trend');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f25dcd",
   "metadata": {},
   "source": [
    "From the above bargraph the highest recorded months with signups were January, July and December. This could probably be as a result of holiday seasons during this specific months. This product sells a bit more, probably due to incentives like discounts offered during the holiday season. Let's plot a bar graph using the special discount column to prove this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly Trend of Special Discount Usage\n",
    "\n",
    "special_discount = df[df['used_special_discount'] == 'Yes']\n",
    "special_discount.groupby('joining_month').size().plot(kind='barh', title='Special Discount Usage per Month');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c49dc5",
   "metadata": {},
   "source": [
    "The bargraph above confirms our assumption that the special discount usage is consistent with our previous findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0553bd6",
   "metadata": {},
   "source": [
    "We are going to do analysis of the average transaction value over the three year period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Behavior Over Time\n",
    "df.groupby('joining_year')['avg_transaction_value'].mean().plot(kind='bar',title='Average Transaction Value by Joining Year');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf17d45",
   "metadata": {},
   "source": [
    "Average transaction value seems to be consistent as it ranges between (25,000-30,000) over the period the data was collected. We will now look at how average transaction value is distrubuted among different regions and age and howit affects churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = {0: '#377EB8', 1: '#E41A1C'}\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=df, x='region_category', y='avg_transaction_value', hue='churn_risk_score', palette=custom_palette, alpha=0.6)\n",
    "plt.title('Transaction Value vs Region Category')\n",
    "plt.xlabel('Region Category')\n",
    "plt.ylabel('Average Transaction Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bfed68",
   "metadata": {},
   "source": [
    "Village category demographic seems to be having the highest average transaction value followed by city then town. Now we will check how it is distributed among the age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d55203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age bands\n",
    "bins = [0, 20, 40, 60, 80, 100]  # Adjust if your age range is different\n",
    "labels = ['0-20', '21-40', '41-60', '61-80', '81-100']\n",
    "df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=df, x='age_group', y='avg_transaction_value', hue='churn_risk_score', palette=custom_palette, alpha=0.6)\n",
    "plt.title('Avg Transaction Value by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Average Transaction Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbdcb18",
   "metadata": {},
   "source": [
    "Average transanction value is evenly distributed across the age groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b236271",
   "metadata": {},
   "source": [
    "We will check the churn and complaints trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75115bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn Risk Score by Year\n",
    "\n",
    "df.groupby('joining_year')['churn_risk_score'].mean().plot(\n",
    "    kind='line',\n",
    "    marker='o',\n",
    "    color='red',\n",
    "    title='Average Churn Risk Score by Joining Year'\n",
    ")\n",
    "\n",
    "plt.xlabel('Joining Year')\n",
    "plt.ylabel('Average Churn Risk Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ba2b9",
   "metadata": {},
   "source": [
    "We can see that the churn risk score decreased in 2016 but increased in 2017. However, there is no clear upward or downward trend, suggesting that the year a customer joined does not strongly influence their likelihood of churning. Further analysis could explore whether specific months, seasons, or other time-based factors have a stronger impact on churn risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn Risk Score by Month\n",
    "df.groupby('joining_month')['churn_risk_score'].mean().plot(\n",
    "    kind='line',\n",
    "    marker='o',\n",
    "    color='red',\n",
    "    title='Average Churn Risk Score by Joining Month'\n",
    ")\n",
    "\n",
    "plt.xlabel('Joining Month')\n",
    "plt.ylabel('Average Churn Risk Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af893343",
   "metadata": {},
   "source": [
    "The absence of a clear upward or downward trend suggests that the month a customer joined does not have a strong, consistent impact on their likelihood of churning. However, the fluctuations in the scores may hint at seasonal or month-specific influences that could warrant further investigation. The month of March and August have the highest risk of churn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097c5f9",
   "metadata": {},
   "source": [
    "Checking the trend of complaints over the period of years the data was collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09942ba",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Complaints Over Time\n",
    "complaints_by_year = df[df['past_complaint'] == 'Yes'].groupby('joining_year').size()\n",
    "complaints_by_year.plot(kind='bar', title='Number of Complaints per Year');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e14fea",
   "metadata": {},
   "source": [
    "From this bargraph above we can see that the number of complaints slightly averaged at over 6,000 for the three years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical distributions\n",
    "for col in ['points_in_wallet']:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(df[col], kde=True,palette='Set1')\n",
    "    plt.title(f\"{col} Distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d99837",
   "metadata": {},
   "source": [
    "From the above graph, we can see that the distribution is uniform. We will check relationship between points in wallet and churn risk to see the effect of financial incentives on customer expenditure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avg Transaction Value vs. Points in Wallet (Scatter Plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df, x='avg_transaction_value', y='points_in_wallet', \n",
    "    hue='churn_risk_score', palette='Set1', alpha=0.6\n",
    ")\n",
    "plt.title('Transaction Value vs. Wallet Points by Churn Risk')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42271940",
   "metadata": {},
   "source": [
    "While the overall spread of transaction values is wide, ranging from near 0 to over 90,000 , most customers tend to cluster around mid-range wallet points (around 500 to 1000). Interestingly, customers who are not at risk of churning appear more concentrated in horizontal bands, particularly at wallet point thresholds like 500 and 1000, suggesting potential reward or loyalty point thresholds that help retain them. In contrast, churn-risk customers are more diffusely scattered, indicating less alignment with those retention-related thresholds. This may imply that higher or more structured wallet point accumulation could play a role in reducing churn risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645abfe7",
   "metadata": {},
   "source": [
    "The below graph shows membership category against churn risk score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b452fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot count of membership category vs churn risk score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='membership_category', hue='churn_risk_score',palette='Set1')\n",
    "\n",
    "# Visuals\n",
    "plt.title(\"Membership Category vs Churn Risk Score\")\n",
    "plt.xlabel(\"Membership Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Churn Risk Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad1b8f",
   "metadata": {},
   "source": [
    "From the above graph we can see that people with basic membership have a high chance of leaving and as the category of membership grows the chance of churn reduces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f039e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=df, x='age', hue='churn_risk_score',fill=False, palette='Set1')\n",
    "\n",
    "plt.title('Age Distribution by Churn Status')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Churn', labels=['No', 'Yes'])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699babb",
   "metadata": {},
   "source": [
    "The age distribution chart shows that customer churn is fairly uniform across all age groups, with no particular age range standing out as having significantly higher or lower churn rates. Both churned and non-churned customers are distributed evenly across the age spectrum, as indicated by the consistent height of the stacked bars and the overlapping KDE lines. This suggests that age is not a strong predictor of churn in this dataset, and other factors such as tenure, usage, or region may offermore meaningful insights into customer churn behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741d396",
   "metadata": {},
   "source": [
    "We will try and see the relationship between churn risk and tenure days using a KDE plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a265d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    # Set analysis reference date (today or dataset's max date)\n",
    "    reference_date = pd.Timestamp.now()  \n",
    "    \n",
    "    # Calculate tenure in days\n",
    "    df['tenure_days'] = (reference_date - df['joining_date']).dt.days\n",
    "    \n",
    "    # Drop rows with invalid/missing dates\n",
    "    df = df.dropna(subset=['tenure_days'])\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(\n",
    "        data=df,\n",
    "        x='tenure_days',\n",
    "        hue='churn_risk_score',\n",
    "        palette='Set1',\n",
    "        fill=False,\n",
    "        common_norm=False,\n",
    "        alpha=0.6\n",
    "    )\n",
    "    plt.title('Customer Tenure Distribution by Churn Risk')\n",
    "    plt.xlabel('Tenure (Days)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Debug Info:\")\n",
    "    print(\"- Unique joining_date types:\", df['joining_date'].apply(type).unique())\n",
    "    print(\"- Sample dates:\", df['joining_date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf75a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a7583",
   "metadata": {},
   "source": [
    "This KDE plot reveals that customers with low churn risk (blue) tend to cluster around 2800 to 3000 days, while high-risk customers (orange) show a broader distribution with a longer tail toward 3400 days, suggesting that while most long-tenured customers are loyal, a subset with very long tenure unexpectedly remains at high churn risk. The overlap around 3000–3200 days indicates tenure alone isn’t a decisive factor, implying the need to combine it with behavioral metrics (e.g.complaints) for accurate churn prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a1515",
   "metadata": {},
   "source": [
    "Checking the complaint status impact using a countplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(\n",
    "    data=df, x='complaint_status', hue='churn_risk_score', \n",
    "    palette='Set1', order=df['complaint_status'].value_counts().index\n",
    ")\n",
    "plt.title('Churn Risk by Complaint Status')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ddd749",
   "metadata": {},
   "source": [
    "The majority of customers fall under the \"Not Applicable\" complaint status, meaning they likely did not raise a complaint, among them, more are at churn risk than not. Interestingly, across all other complaint categories; whether the complaint was \"Unsolved\", \"Solved\", \"Solved in Follow-up\", or \"No Information Available\"; the count of customers at churn risk consistently exceeds those not at risk. This suggests that regardless of whether a complaint was resolved, simply having a complaint recorded correlates with higher churn risk. Even when complaints were marked as solved, the churn risk remains relatively high, highlighting a potential gap in customer satisfaction or follow-through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7133b",
   "metadata": {},
   "source": [
    "We will now check the relationship between churn risk score and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# point plot creation of region_category vs churn_risk_score\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.pointplot(\n",
    "    data=df, \n",
    "    x='region_category', \n",
    "    y='churn_risk_score', \n",
    "    errorbar=('ci', 95),  \n",
    "    capsize=0.1,\n",
    "    palette='Set2'\n",
    ")\n",
    "plt.title('Average Churn Risk by Region', pad=15)\n",
    "plt.xlabel('Region Category')\n",
    "plt.ylabel('Average Churn Risk Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ad2ba",
   "metadata": {},
   "source": [
    "The highest average churn risk is observed among customers in City regions, followed closely by those in Towns, while Villages show the lowest average churn risk. The relatively tight confidence intervals suggest these differences are statistically meaningful, especially between Village and City. This pattern may indicate that customers in more urban areas are more likely to consider switching or disengaging, possibly due to more alternatives or higher service expectations. These insights could help guide targeted retention strategies based on customer location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b446416",
   "metadata": {},
   "source": [
    "## DATA CLEANING\n",
    "\n",
    "Starting with preferred_offer_types column we shall proceed to remove the null values. Since the percentage of null values in this column is 0.77% which is almost negligible, we will drop all the null rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7649a8",
   "metadata": {},
   "source": [
    "We decided to drop the security number and ID columns since they did not offer any useful insightinto our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['security_no', 'last_visit_time', 'referral_id'], inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e795d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'preferred_offer_types' is null\n",
    "df_cleaned = df.dropna(subset=['preferred_offer_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most frequent value (mode) of 'region_category'\n",
    "mode_region = df_cleaned['region_category'].mode()[0]\n",
    "print(f\"The most frequent (mode) value in 'region_category' is: {mode_region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd128907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of 'region_category'\n",
    "region_distribution = df_cleaned['region_category'].value_counts(dropna=False)\n",
    "\n",
    "# Display the result\n",
    "print(\"Distribution of values in 'region_category':\")\n",
    "print(region_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'region_category' is null\n",
    "df_cleaned = df_cleaned.dropna(subset=['region_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result\n",
    "null_percentage_cleaned = (df_cleaned.isnull().sum() / len(df_cleaned)) * 100\n",
    "print(null_percentage_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f3dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of 'region_category'\n",
    "points_in_wallet_distribution = df_cleaned['points_in_wallet'].value_counts(dropna=False)\n",
    "\n",
    "# Display the result\n",
    "print(\"Distribution of values in 'points_in_wallet':\")\n",
    "print(points_in_wallet_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b402a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "df_cleaned['points_in_wallet'].fillna(df_cleaned['points_in_wallet'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49445c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentage_cleaned = (df_cleaned.isnull().sum() / len(df_cleaned)) * 100\n",
    "print(null_percentage_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b54677",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a1087",
   "metadata": {},
   "source": [
    "We've observed that the dataset contains many hidden missing values represented by placeholder terms. To accurately identify and address them, we plan to carefully inspect each column individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ae166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the membership_categoty column\n",
    "df_cleaned['membership_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2256407",
   "metadata": {},
   "source": [
    "For this column, the values appear valid and present; there are no hidden missing values detected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba33f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values in the joining_date column\n",
    "df_cleaned[df_cleaned['joining_date'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the joined through referral column\n",
    "df_cleaned['joined_through_referral'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579384b8",
   "metadata": {},
   "source": [
    "We have identified several entries where the value is missing and represented as '?'. Since this is a boolean field, it is difficult to confidently classify these as either 'Yes' or 'No'. Therefore, we will replace these unknown values with 'Not_recorded' to maintain clarity without making inaccurate assumptions or losing data in dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f338773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing '?' with 'Not_recorded'\n",
    "df_cleaned['joined_through_referral'] = df_cleaned['joined_through_referral'].replace('?','Not_recorded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac374ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the age column\n",
    "df_cleaned['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6783b",
   "metadata": {},
   "source": [
    "The column age seems not have any data that is misclassified therefore we will move onto the gender column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a74064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the gender column\n",
    "df_cleaned['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062b23e",
   "metadata": {},
   "source": [
    "The number of rows with gender = 'Unknown' is negligible (~0.16%) and may introduce noise or ambiguity in gender-based analysis or modeling. Dropping them helps ensure clean, interpretable categorical data without significantly affecting the dataset's size or balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15547ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where gender is 'Unknown' in place\n",
    "df_cleaned = df_cleaned[df_cleaned['gender'] != 'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the region_category column\n",
    "df_cleaned['region_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4426ab",
   "metadata": {},
   "source": [
    "The region_category column does not appear to have any misclassified data based on the value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the region_category column\n",
    "df_cleaned['preferred_offer_types'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac56c3",
   "metadata": {},
   "source": [
    "The column preferred_offer_types doesn't seem to have any data that is misclassified therefore we will leave it as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a35036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the medium_of_operation column\n",
    "df_cleaned['medium_of_operation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf1716",
   "metadata": {},
   "source": [
    "We will replace the rows with the value ? with the word unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing rows with ? with the unknown\n",
    "df_cleaned['medium_of_operation'] = df_cleaned['medium_of_operation'].replace('?', 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the internet_option column\n",
    "df_cleaned['internet_option'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f32485",
   "metadata": {},
   "source": [
    "The column internet_option doesn't seem to have any data that is misclassified therefore we will leave it as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the days_since_last_login column\n",
    "df_cleaned['days_since_last_login'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f11f3",
   "metadata": {},
   "source": [
    "We can see that there is an outlier value in -999. We will first replace it with null values then replace the null values with median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb962960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -999 with NaN first\n",
    "df_cleaned['days_since_last_login'] = df_cleaned['days_since_last_login'].replace(-999, np.nan)\n",
    "\n",
    "# Fill NaN with the median\n",
    "median_login = df_cleaned['days_since_last_login'].median()\n",
    "df_cleaned['days_since_last_login'].fillna(median_login, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4722e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the avg_time_spent  column\n",
    "df_cleaned['avg_time_spent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the unique values for avg_time_spent column\n",
    "unique_avg_time_spent = df_cleaned['avg_time_spent'].unique()\n",
    "\n",
    "# Display the result\n",
    "print(\"Unique average time spent values:\")\n",
    "print(unique_avg_time_spent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822b9ba",
   "metadata": {},
   "source": [
    "We can see that there values recorded as negative. We will print the values to get a better perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and show unique values less than 0\n",
    "negative_values = df_cleaned[df_cleaned['avg_time_spent'] < 0]['avg_time_spent'].unique()\n",
    "\n",
    "print(\"Unique values in 'avg_time_spent' that are less than 0:\")\n",
    "print(negative_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd13af",
   "metadata": {},
   "source": [
    "we will then check how many rows are affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of negative rows\n",
    "num_negatives = (df_cleaned['avg_time_spent'] < 0).sum()\n",
    "print(f\"Number of rows with negative avg_time_spent: {num_negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68578e0a",
   "metadata": {},
   "source": [
    "we will replace the negative values with a median value retrived from positive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace with median of positive values\n",
    "median_time = df_cleaned[df_cleaned['avg_time_spent'] >= 0]['avg_time_spent'].median()\n",
    "df_cleaned.loc[df_cleaned['avg_time_spent'] < 0, 'avg_time_spent'] = median_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946cddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the average transaction value column\n",
    "\n",
    "df_cleaned['avg_transaction_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85967cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the unique values for average transaction value column\n",
    "unique_avg_transaction_value = df_cleaned['avg_transaction_value'].unique()\n",
    "print(unique_avg_transaction_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bca6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['points_in_wallet'] = df_cleaned['points_in_wallet'].astype(str).str.strip()\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert points_in_wallet to numeric\n",
    "df_cleaned['points_in_wallet'] = pd.to_numeric(df_cleaned['points_in_wallet'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null vaules filled with median\n",
    "df_cleaned['points_in_wallet'].fillna(df_cleaned['points_in_wallet'].median(), inplace=True)\n",
    "df_cleaned['used_special_discount'] = df_cleaned['used_special_discount'].astype(str).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b83fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_mapping = {\n",
    "    'yes': 'Yes',\n",
    "    'no': 'No',\n",
    "    'y': 'Yes',\n",
    "    'n': 'No'\n",
    "}\n",
    "df_cleaned['used_special_discount'] = df_cleaned['used_special_discount'].map(discount_mapping).fillna('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef162cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['offer_application_preference'] = df_cleaned['offer_application_preference'].astype(str).str.strip().str.lower()\n",
    "offer_mapping = {\n",
    "    'yes': 'Yes',\n",
    "    'no': 'No',\n",
    "    'y': 'Yes',\n",
    "    'n': 'No'\n",
    "}\n",
    "df_cleaned['offer_application_preference'] = df_cleaned['offer_application_preference'].map(offer_mapping).fillna('No')\n",
    "df_cleaned['past_complaint'] = df_cleaned['past_complaint'].astype(str).str.strip().str.lower()\n",
    "complaint_mapping = {\n",
    "    'yes': 'Yes',\n",
    "    'no': 'No',\n",
    "    'y': 'Yes',\n",
    "    'n': 'No'\n",
    "}\n",
    "df_cleaned['past_complaint'] = df_cleaned['past_complaint'].map(complaint_mapping).fillna('No')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5494e6a",
   "metadata": {},
   "source": [
    "Before we apply scaling or modeling, we need to make sure all our numeric fields are clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string-like errors to numeric and handle missing values\n",
    "cols_to_clean = ['avg_frequency_login_days', 'avg_transaction_value', 'points_in_wallet',\n",
    "                 'days_since_last_login', 'avg_time_spent']\n",
    "\n",
    "for col in cols_to_clean:\n",
    "    df_cleaned[col] = pd.to_numeric( df_cleaned[col], errors='coerce')\n",
    "    df_cleaned[col].fillna( df_cleaned[col].median(), inplace=True)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95e86a",
   "metadata": {},
   "source": [
    "All numeric columns are clean and free of errors or missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fbf9d6",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dfafbe",
   "metadata": {},
   "source": [
    "### STANDARDIZATION  \n",
    "\n",
    "Some of the features like `avg_time_spent`, `avg_transaction_value`,  are on different scales.To ensure fair treatment by models, we’ll **standardize** to centers the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbad49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the dataframe for standardizatiom\n",
    "df_standardized = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1668e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_cols = ['avg_time_spent', 'avg_transaction_value']\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "# Fit and transform\n",
    "df_standardized_cols = pd.DataFrame(\n",
    "    scaler_std.fit_transform(df_standardized[standardize_cols]),\n",
    "    columns=[f\"{col}_std\" for col in standardize_cols],\n",
    "    index=df_standardized.index)\n",
    "\n",
    "# Concatenate\n",
    "df_standardized = pd.concat([df_standardized, df_standardized_cols], axis=1)\n",
    "df_standardized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for i, col in enumerate(standardize_cols):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(df_standardized[col], kde=True, bins=30)\n",
    "    plt.title(f\"{col} - Original\")\n",
    "    plt.xlim(df[col].min(), df[col].max())\n",
    "\n",
    "    plt.subplot(2, 3, i+4)\n",
    "    sns.histplot(df_standardized[f\"{col}_std\"], kde=True, palette='Set1', bins=30)\n",
    "    plt.title(f\"{col}_std - Standardized\")\n",
    "    plt.xlim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Standardization: Before vs After\", fontsize=16, y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65ac46",
   "metadata": {},
   "source": [
    "After standardization, the features are centered.Preparing them well for models sensitive to feature magnitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9fb61",
   "metadata": {},
   "source": [
    "### NORMALIZATION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9fe32",
   "metadata": {},
   "source": [
    "### Normalize Features to Range [0, 1]\n",
    "\n",
    "Normalization rescales features, We  will use it for  this features `days_since_last_login`, `avg_frequency_login_days`, and `points_in_wallet`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7778c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Columns to normalize\n",
    "normalize_cols = ['avg_frequency_login_days', 'points_in_wallet']\n",
    "\n",
    "# Convert to numeric and handle errors\n",
    "df_standardized[normalize_cols] = df_standardized[normalize_cols].apply(pd.to_numeric, errors='coerce')\n",
    "df_standardized[normalize_cols] = df_standardized[normalize_cols].fillna(0)\n",
    "\n",
    "# Normalize\n",
    "scaler_norm = MinMaxScaler()\n",
    "df_standardized_norm = pd.DataFrame(\n",
    "    scaler_norm.fit_transform(df_standardized[normalize_cols]),\n",
    "    columns=[f\"{col}_norm\" for col in normalize_cols],\n",
    "    index=df_standardized.index\n",
    ")\n",
    "# Combining\n",
    "df_standardized = pd.concat([df_standardized, df_standardized_norm], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing before and after normalization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, col in enumerate(normalize_cols):\n",
    "    # Original\n",
    "    plt.subplot(2, len(normalize_cols), i + 1)\n",
    "    sns.histplot(df_standardized[col], kde=True)\n",
    "    plt.title(f\"{col} - Original\")\n",
    "\n",
    "    # Normalized\n",
    "    plt.subplot(2, len(normalize_cols), i + 1 + len(normalize_cols))\n",
    "    sns.histplot(df_standardized[f\"{col}_norm\"], kde=True, palette='Set1')\n",
    "    plt.title(f\"{col}_norm - Normalized\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Before and After Normalization\", fontsize=14, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16282bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized[['avg_frequency_login_days_norm', 'points_in_wallet_norm']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ff4fd",
   "metadata": {},
   "source": [
    "Normalization didn't change the shape from the visualizations, which is\n",
    "expected. We confirmed normalization worked correctly by checking `.describe()` stats and the new range (0–1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35346a",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a4387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a3691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the dataframe for encoding\n",
    "df_encoded = df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all the categorical columns we have\n",
    "categorical_cols = df_encoded.select_dtypes(include='object').columns.to_list()\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf85cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[categorical_cols].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd9777",
   "metadata": {},
   "source": [
    "For columns that are binary or represent a clear yes/no scenario, we will apply binary encoding to keep the analysis simple and easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a binary map\n",
    "binary_map = {\n",
    "    'gender': {'F': 0, 'M': 1},\n",
    "    'joined_through_referral': {'Yes': 1, 'No': 0, 'Not_recorded': -1},\n",
    "    'used_special_discount': {'Yes': 1, 'No': 0},\n",
    "    'offer_application_preference': {'Yes': 1, 'No': 0},\n",
    "    'past_complaint': {'Yes': 1, 'No': 0}\n",
    "}\n",
    "# Apply binary encoding\n",
    "for col, mapping in binary_map.items():\n",
    "  df_encoded[col] = df_encoded[col].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7b08e",
   "metadata": {},
   "source": [
    "For the rest of the columns, we will use one-hot encoding since they all have low cardinality and this approach preserves the interpretability of the data without introducing unnecessary complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a857fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe for the remaining columns\n",
    "one_hot_columns = ['region_category', 'membership_category', 'preferred_offer_types', 'medium_of_operation', 'internet_option', 'complaint_status', 'feedback']\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=one_hot_columns, prefix=one_hot_columns, dtype=int)\n",
    "# manually selecting which columns to drop to avoid multicollinearity but still have control over what is being removed\n",
    "to_drop = [\n",
    "    'region_category_Village',\n",
    "    'membership_category_No Membership',\n",
    "    'preferred_offer_types_Without Offers',\n",
    "    'medium_of_operation_Unknown',\n",
    "    'internet_option_Fiber_Optic',\n",
    "    'complaint_status_No Information Available',\n",
    "    'feedback_No reason specified','joining_date',\n",
    "    'points_in_wallet'\n",
    "    ]\n",
    "# dropping the chosen columns\n",
    "df_encoded = df_encoded.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361ad2f",
   "metadata": {},
   "source": [
    "## Modeling \n",
    "\n",
    "We decided to go for Logistic regression, random forest, XG boost and light GBM because we are dealing with binary data(Churn risk) and this four models are appropriate for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84063a4",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e87ae3",
   "metadata": {},
   "source": [
    "#### Why Logistic Regression? \n",
    "\n",
    "Predicting customer churn risk using features such as referral status, membership category, internet usage, and complaint history. \n",
    "  we started with Logistic Regression for a few key reasons: Easy to interpret , and also provides a simple baseline to compare with more complex models later \n",
    "\n",
    "The goal is to:\n",
    "- Identify features strongly linked to churn.\n",
    "- Understand the relationship between customer behavior and churn risk.\n",
    "- Evaluate how well a simple model can perform before going deeper into tuning and ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381421bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop target and non-numeric column joining data\n",
    "X = df_encoded.drop(columns=['churn_risk_score']) \n",
    "# defining our dependent variable as churn_risk_score\n",
    "y = df_encoded['churn_risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72840e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a split train test for our logistic regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cd8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000) \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d02c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting confusion matrix of our logistic regression model\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='coolwarm', values_format='d')\n",
    "\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed3517",
   "metadata": {},
   "source": [
    "The confusion matrix and classification report show that the logistic regression model performs reliably in predicting customer churn risk. Out of 6,255 test samples, it correctly identified 2,342 low-risk (true negatives) and 2,929 high-risk customers (true positives), while misclassifying 540 low-risk as high-risk (false positives) and 444 high-risk as low-risk (false negatives). The precision for both classes is 0.84, meaning the model is accurate 84% of the time when it makes a prediction. Recall is 0.81 for low-risk and 0.87 for high-risk customers, showing the model is especially good at identifying actual churners. With F1-scores of 0.83 for low-risk and 0.86 for high-risk, there's a solid balance between precision and recall. Overall, the model hits an accuracy of 84%, and the macro and weighted averages confirm consistent performance across both classes. This makes the model a strong and balanced tool for identifying churn risk in customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6346ede",
   "metadata": {},
   "source": [
    "We will now plot a bar plot ranking how each feature affected our logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b534808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and coefficients\n",
    "feature_names = X_train.columns\n",
    "coefficients = model.coef_[0]  # For binary classification, only one row\n",
    "\n",
    "# Create a DataFrame for easy sorting\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Get absolute values to measure impact magnitude\n",
    "importance_df['Abs_Coefficient'] = importance_df['Coefficient'].abs()\n",
    "\n",
    "# Sort by absolute value\n",
    "importance_df = importance_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Get Set1 color palette from seaborn\n",
    "colors = sns.color_palette(\"coolwarm\", n_colors=20)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 14))  # increase height from 8 to 14\n",
    "plt.barh(importance_df['Feature'][:20][::-1], importance_df['Coefficient'][:20][::-1], color=colors)\n",
    "plt.axvline(0, color='grey', linestyle='--')\n",
    "plt.title('Top 20 Most Influential Features (Logistic Regression)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c8709",
   "metadata": {},
   "source": [
    "The most influential feature by far is being in the Basic Membership category, which has a strong positive coefficient, indicating that these customers are significantly more likely to churn. In contrast, higher-tier memberships like Premium, Platinum, Gold, and Silver have large negative coefficients, suggesting that customers in these categories are much more loyal and less likely to leave. Negative feedback such as Poor Product Quality, Poor Customer Service, Poor Website Experience, and Too Many Ads also shows a strong positive impact on churn, meaning customers expressing these concerns are more likely to drop off. In contrast, positive feedback—like finding the website user-friendly, appreciating reasonable prices, or experiencing good customer care—correlates with a lower likelihood of churn. Other factors like points in wallet, preferred offer types, and even medium of operation (e.g., Desktop users) also contribute, though to a lesser degree. Interestingly, customers who joined through referral or had past complaints show a slight increase in churn risk. Overall, the model emphasizes that churn is driven both by membership tier and customer experience, pointing to clear areas for retention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8a848",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e5006",
   "metadata": {},
   "source": [
    "#### Why use  Random Forest? \n",
    "\n",
    "After testing Logistic Regression as a baseline, we applied Random Forest to capture more complex patterns in the data. because it Handles both numerical and categorical data without scaling, Captures non-linear relationships that Logistic Regression might miss. Reduces overfitting by averaging multiple decision trees. Provides feature importance to identify key churn indicators. \n",
    "\n",
    "we aim Improve prediction accuracy, Identify the most influential features., Evaluate the benefits of using ensemble methods. Random Forest helps strike a balance between performance and interpretability, making it a solid next step in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model \n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test, cmap='coolwarm')\n",
    "\n",
    "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd75c2",
   "metadata": {},
   "source": [
    "The Random Forest Classifier has demonstrated strong performance in predicting customer churn risk, achieving an overall accuracy of approximately 93.22%. According to the classification report, the model shows high precision and recall for both classes. For low-risk customers (class 0), the precision is 0.95 and recall is 0.91, meaning that the model accurately predicts non-churners most of the time but misses a small portion. For high-risk customers (class 1), the precision is 0.92 and recall is 0.96, indicating the model is especially effective at identifying customers who are likely to churn. The F1-scores of 0.92 for class 0 and 0.94 for class 1 confirm a strong balance between precision and recall across both groups. The macro and weighted averages are all at 0.93, reflecting consistent and balanced performance. Overall, the model appears to be highly reliable and well-suited for churn prediction tasks on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efc679",
   "metadata": {},
   "source": [
    "We will draw a barchart to analyse feature importance of the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "# Create a DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot top 20\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df['Feature'][:20][::-1], importance_df['Importance'][:20][::-1], color='blue')\n",
    "plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10242f96",
   "metadata": {},
   "source": [
    "The feature with the highest importance by far is points in wallet (0.35), indicating that the number of points a customer has in their wallet plays a major role in determining churn risk. This suggests that customers with more loyalty or reward points are less likely to churn, possibly because they’re more engaged or have incentives to stay.\n",
    "\n",
    "Next in line are membership categories, especially Premium (0.10), Platinum (0.098), and Basic (0.079). These indicate that membership level strongly influences churn behavior, with different tiers carrying different retention patterns. For example, Premium and Platinum members might have more benefits, making them less likely to leave, whereas Basic members might be more likely to churn.\n",
    "\n",
    "Other transactional features like average transaction value and its standard deviation also show up as important (~0.04 each), meaning spending behavior has a solid link to churn likelihood. Similarly, Gold and Silver memberships contribute to the prediction, though with slightly less impact.\n",
    "\n",
    "Time-related features like tenure_days, avg_time_spent, and avg_time_spent_std also appear in the list, highlighting that how long and how actively a customer interacts with the platform can affect churn probability. Login frequency and recency (avg_frequency_login_days_norm, days_since_last_login) also play a role, albeit smaller, indicating that engagement levels matter.\n",
    "\n",
    "Interestingly, customer feedback categories such as \"Reasonable Price\", \"User Friendly Website\", and \"Products always in Stock\" also contribute meaningfully. This suggests that positive experiences and satisfaction with the platform influence churn decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ca3a4",
   "metadata": {},
   "source": [
    "### XG Boost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "X = df_encoded.drop('churn_risk_score', axis=1)\n",
    "y = df_encoded['churn_risk_score'] \n",
    "\n",
    "# A standard 80-20 split ensures enough training data while maintaining a sufficiently large test set for reliable evaluation.\n",
    "# Splitting data into train and test sets using 80-20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "# we will use binary:logistics because our target variable is binary\n",
    "    objective='binary:logistic',\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_probability = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb914099",
   "metadata": {},
   "source": [
    "The XGBoost model achieved an accuracy of 93.4%, indicating strong overall performance in predicting customer churn. The classification report reveals balanced precision and recall scores for both classes (0: non-churn, 1: churn). A recall of 0.91 for non-churn and 0.95 for churn suggests the model is slightly better at correctly identifying customers who will churn while maintaining high precision for both classes. The F1-scores confirm robust performance across metrics, with no significant class imbalance issues evident in the results. However to reduce false negatives (missed churn cases), the recall for class 1 could potentially be improved further through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(xgb_model, X_test, y_test, cmap='coolwarm')\n",
    "\n",
    "plt.title(\"Confusion Matrix - XGBoost\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d23c0d",
   "metadata": {},
   "source": [
    "This confusion matrix shows the model correctly predicted 2,626 non-churn cases and 3,216 churn cases, with 157 false positives and 257 false negatives. The higher true positive rate aligns with the classification report’s 95% recall for churn, indicating strong detection of at-risk customers. However, the false negatives (257) represent missed opportunities for intervention, which could be addressed by hyperparameter tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(feature_importance)), feature_importance[sorted_idx], align='center')\n",
    "plt.xticks(range(len(feature_importance)), np.array(X.columns)[sorted_idx], rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6c8c3",
   "metadata": {},
   "source": [
    "This feature importance analysis reveals that membership category (especially Basic and Premium) and points in wallet are the strongest predictors of churn, with high importance scores (0.125–0.075). Other influential factors include transaction value, customer feedback, and login behavior. Notably, operational factors like medium_of_operation_Smartphone and unresolved complaints also contribute, though with lower impact. This suggests that subscription tier, financial incentives, and service quality are key drivers of customer retention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e70b8ab",
   "metadata": {},
   "source": [
    "We will now improve this XG Boost model using RandomizedSearchCV. We chose RandomizedSearchCV because it efficiently finds strong hyperparameter combinations without the computational cost of exhaustive grid searches, making it ideal for optimizing XGBoost's many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376927fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\n",
    "# Defining the parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1),\n",
    "    'scale_pos_weight': [1, y_train.value_counts()[0]/y_train.value_counts()[1]]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "xgb_tuned = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=40,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Executing search\n",
    "xgb_tuned.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", xgb_tuned.best_params_)\n",
    "print(\"Best F1 score:\", xgb_tuned.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = xgb_tuned.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87910f33",
   "metadata": {},
   "source": [
    "The tuned model performs nearly the same as the original one, with both achieving 93% accuracy and similar F1-scores. The main difference is that the tuned model uses simpler, smaller trees with stronger regularization—meaning it’s less likely to overfit while keeping the same high performance. Since the results are almost identical, the original model was already well-optimized, and the tuning just confirmed that. Either version would work well for predicting churn and we can confidently deploy either version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b95c7b",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab95a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create base MLP model\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train model with best parameters\n",
    "best_mlp = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = best_mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance (for interpretation)\n",
    "# Note: MLPs don't directly provide feature importance, but we can use permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(best_mlp, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Create a DataFrame with feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04469db3",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Our objective was to build a supervised machine learning model capable of predicting customer churn risk for internet service providers using behavioral, transactional, and feedback data. We managed to build models using ensemble methods and neural networks and ended up with 93% accuracy in predicting customer churn which is XG-boost.\n",
    "\n",
    "This analysis aimed at answering the following business questions:\n",
    "\n",
    "- Which *customer categories* are most likely to churn?\n",
    "  Our analysis showed that Basic Membership holders have the highest likelihood of churning. As membership tiers increase (Silver, Gold,       Premium, Platinum), the churn risk significantly drops. This points to membership level being a major driver of customer retention.\n",
    "   \n",
    "- What are the top *behavioral or demographic* predictors of churn?\n",
    "  From a demographic perspective, customers living in City regions have the highest average churn risk, followed by Towns, with Villages       having the lowest risk. Interestingly, rural (village) customers also show higher average transaction values, hinting at their potential     long-term value. This suggests that targeted retention strategies should focus more on urban areas.\n",
    "       \n",
    "- Can *sentiment in customer feedback* help predict churn risk?\n",
    "  We found that just having a recorded complaint, regardless of whether it was resolved, is linked to higher churn risk. Even resolved         complaints still correlate with elevated churn, indicating possible issues with the quality of resolution or post-resolution experience.     Feedback clearly plays a major role in shaping customer loyalty. \n",
    "\n",
    "- How effective are current *loyalty strategies* in retaining customers?\n",
    "  We saw that non-churning customers tended to cluster around wallet point values of 500–1000, suggesting that these thresholds might align    with retention-boosting benefits. In contrast, churn-risk customers showed scattered wallet point values, indicating weaker engagement       with the loyalty system. More structured or visible rewards could improve retention.\n",
    "\n",
    "  \n",
    "- What *actionable retention strategies* can we derive from high churn risk profiles?\n",
    "  1. Encourage membership upgrades by offering exclusive perks or discounts for higher-tier plans.\n",
    "  2. Improve the complaint resolution process, not just technically, but in terms of customer experience; faster, clearer follow-up could         reduce dissatisfaction. In addition, send follow-up surveys or confirmation messages that assure customers they’ve been heard.\n",
    "  3. The company could introduce incentives such as tiered rewards, point boosters, and milestone badges to increase wallet point engagement.\n",
    "  4. Use churn-risk predictions to trigger targerted ad campaigns such as special offers, call-backs, or targeted emails.\n",
    "  5. Focus efforts in city and town regions where churn risk is higher, possibly with local partnerships or targeted discounts.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d932d61",
   "metadata": {},
   "source": [
    "## Deployment of the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e736c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Saving the model\n",
    "joblib.dump(model, 'churn_model.pkl')\n",
    "\n",
    "# Loading the model\n",
    "loaded_model = joblib.load('churn_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc0345-cc5f-4363-b9c5-e870dc1f4272",
   "metadata": {},
   "source": [
    "Deployment uses XGBoost as the best model, saved with joblib, and served through a Flask API. The model is loaded into the app, which exposes a /predict route that accepts JSON input and returns predictions. This setup enables quick and efficient model deployment as a web service, testable via tools like Postman or curl."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1K0lMlXufygclUYlZst3rF3jVt94SYxqq",
     "timestamp": 1747923311027
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
